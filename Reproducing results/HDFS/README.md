Reproducing Results for HDFS
=======
The scripts in this directory can be used to reproduce HDFS results that are presented in paper titled "Toward a Generic Fault Tolerance Technique for Partial Network Partitioning" that is published in OSDI'20. Results in the paper were generated by running the experiments on 16 xl170 nodes in Utah cluster on CloudLab.

Prerequesets
-------
1- Install Hadoop from the website: http://www.apache.org/dyn/closer.cgi/hadoop/common/. Make sure you have an appropriate Java version as described in https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions.

2- Install the Paramiko library for Python SSH, the easiest way is using pip.
```bash
$ pip install paramiko
```
While most environments already have pip, you may need to install it manually as described in https://github.com/pypa/get-pip.


3- We assume that the machines in the experiment can SSH into each other, or at least one controller node can SSH into all machines. This could call for setting up some keys for SSH.

Running the Experiment
-------
1- Start by setting HDFS parameteres. In our experiments, we used default configurations for everything, except for setting paths for directories for hadoop to use in $HADOOP_HOME/etc/hadoop/hdfs-site.xml. The files should have the following two properties at least, with your desired [HADOOP_STORE_DIR] that exists on all cluster nodes:

```xml
     <property>
            <name>dfs.namenode.name.dir</name>
            <value>file:[HADOOP_STORE_DIR]/hadoop_store/hdfs/namenode</value>
     </property>
     <property>
            <name>dfs.datanode.data.dir</name>
            <value>file:[HADOOP_STORE_DIR]/hadoop_store/hdfs/datanode</value>
     </property>
```

2- Set variables in the config.py file: 
* HADOOP_HOME: making sure that this directory is where hadoop is installed and is the same for all nodes in the experiment
* HADOOP_STORE: this should be set to the [HADOOP_STORE_DIR] mentioned in step 1.
* The IP addresses of all nodes in the experiment, this includes the HDFS cluster nodes (NameNodes and DataNodes) and the nodes which will run the benchmark client.
* The size of the HDFS cluster, which will be split into 1 NameNode and the rest will be DataNodes.

3- From the controller node, which could be a separate node or part of the cluster, start HDFS. You can use:
```bash
$ python start_hdfs.py
```
The script will start a NameNode on the first IP address machine in the list (in config.py), and enough DataNodes to satify the set cluster size.

4- If you're testing with Nifty, now would be the time to start it using deploy/deploy-nifty.sh. You can learn more on how to start Nifty in the Readme of the deploy directory.

5- Run the HDFS TestDFSIO benchmark. This can be done by:
```bash
$ python run_benchmark.py <number_of_clients>
```
Clients will be distributed onto the machines that are in the experiment but were not used in the HDFS cluster. The client will run in parallel, starting with a clean up, then writing to the HDFS cluster, then reading the same files they wrote. The run_benchmark script then returns the total throughput of the cluster in the write period and in the read period.

To recreate results faster, you might want to use the following command:
```bash
$ python run_exp.py <init_num_of_clients> <final_num_of_clients> <step_size>
```
This command will create a CSV file called results.csv with the Read and Write throughputs across different client counts.


Reproducing Results
------
In order to reproduce the results in the paper, you have to run the experiment (as described above) with and without Nifty and compare the results. Please refer the deploy folder to check how to deploy Nifty.
